# Pattern Recognition
## Dimensionality Reduction, ì°¨ì› ì¶•ì†Œ
### Curse odimensionality, ì°¨ì›ì˜ ì €ì£¼
- featureê°€ ëŠ˜ì–´ë‚  ìˆ˜ë¡ ì„±ëŠ¥ì€ ì¢‹ì•„ì§€ì§€ë§Œ, ë” ì´ìƒ ì¶”ê°€í•´ë„ ì„±ëŠ¥ì´ ê¸‰ê°í•˜ëŠ” ê³³ì´ ìˆë‹¤. training dataì—ë§Œ overfittingë˜ê³ , complexityë„ ë†’ì•„ì§€ê¸° ë•Œë¬¸ì´ë‹¤.
- irrelevant(ë¬´ê´€ê³„)í•˜ê±°ë‚˜ redundant(ì¤‘ë³µ)í•œ featureë“¤ì„ ì˜ ê³¨ë¼ë‚´ì•¼ í•œë‹¤.

### Number of sample
- ë³´í†µ featureê°€ 10ë°° ë” ë§ì•„ì•¼ í•˜ì§€ë§Œ, classifierê°€ complexí•  ìˆ˜ë¡ sample/feature ratioê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ë” ë†’ì•„ì•¼ í•œë‹¤.

### `Dimensionality Reduction, ì°¨ì› ì¶•ì†Œ`
- ê·¸ë ‡ë‹¤ë©´ featureë¥¼ ì–´ë–»ê²Œ ì¤„ì¼ ìˆ˜ ìˆì„ê¹Œ? dimensionì„ reduceí•˜ì.
1. appropriate `subset`ì„ ì°¾ì•„ë‚´ì.
2. `Transforming`(Combining)í•˜ì.

## 1. Feature Selection
### Feature Selection
- Exhaustive searchëŠ” impracticalí•˜ë¯€ë¡œ, í¬ê²Œ ì•„ë˜ ë‘ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤.
- a. `Filter`
<br> Objective functionì€ classifierì™€ ë…ë¦½ì ì´ë©°, class ê°„ ê±°ë¦¬ë‚˜, í†µê³„ì  ì˜ì¡´ì„± ë“±ë§Œì„ ê³ ë ¤í•œë‹¤.
<br> ì†ë„ëŠ” ë¹ ë¥´ì§€ë§Œ, í•¨ìˆ˜ê°€ ë‹¨ì¡°ë¡œì›Œ ë†’ì€ ì„±ëŠ¥ì„ ê¸°ëŒ€í•˜ê¸°ëŠ” ì–´ë µë‹¤.
- b. `Wrapper` (ì•„ë˜ ì˜ˆì‹œë“¤ì€ ëŒ€ë¶€ë¶„ Wrapper ë°©ì‹ìœ¼ë¡œ ì§„í–‰ë¨)
<br> Objective functionì€ classification algorithmì„ ì‚¬ìš©í•˜ë©°, predictive accuracyë¡œ íŒë‹¨í•œë‹¤.
<br> ì†ë„ëŠ” ëŠë¦¬ì§€ë§Œ, classifierì— fití•˜ë‹¤.

### `Naive search`
- performance ìˆœì„œëŒ€ë¡œ ê¸°ëŠ¥ ì •ë ¬í•œ ë’¤, `ìƒìœ„ mê°œ Feature ì„ íƒ`í•œë‹¤.
- ë‹¨, feature correlation(or dependence)ëŠ” ê³ ë ¤ë˜ì§€ ì•ŠëŠ”ë‹¤.

### Sequential Forward Selection (`SFS`)
- ê³¼ì •
<br> 1. Empty subsetì—ì„œ ì‹œì‘
<br> 2. 1ê°œ ì¶”ê°€í–ˆì„ë•Œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì¸ featureë¥¼ ì°¾ìŒ
<br> 3. í•´ë‹¹ featureë¥¼ ì§‘í•©ì— í¬í•¨ì‹œí‚¨ subsetì„ ì±„íƒ
<br> 4. 2ë²ˆìœ¼ë¡œ ê° (optimalì¼ ë•Œê¹Œì§€ ë°˜ë³µ)
- optimal subsetì´ ì‘ì„ ë•Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë‹¨ì ìœ¼ë¡œ í•œë²ˆ ì¶”ê°€í•œ featureëŠ” ì œê±°í•  ìˆ˜ ì—†ë‹¤.

### Sequential Backward Selection (`SBS`)
- ê³¼ì •
<br> 1. Full subsetì—ì„œ ì‹œì‘
<br> 2. 1ê°œ ì œê±°í–ˆì„ë•Œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì¸ featureë¥¼ ì°¾ìŒ
<br> 3. í•´ë‹¹ featureë¥¼ ì§‘í•©ì—ì„œ ì œê±°ì‹œí‚¨ subsetì„ ì±„íƒ
<br> 4. 2ë²ˆìœ¼ë¡œ ê° (optimalì¼ ë•Œê¹Œì§€ ë°˜ë³µ)
- optimal subsetì´ í´ ë•Œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ë©°, ë‹¨ì ìœ¼ë¡œ í•œë²ˆ ì œê±°í•œ featureëŠ” ì¶”ê°€í•  ìˆ˜ ì—†ë‹¤.

### Bidirectional Search (`BDS`)
- SFSì™€ SBSë¥¼ ê°™ì´ ì‚¬ìš©í•œë‹¤.
<br> SFSì—ì„œ ì„ íƒí•œ FeatureëŠ” SBSì—ì„œ ì œê±°í•˜ì§€ ì•Šìœ¼ë©°,
<br> SBSì— ì œê±°í•œ FeatureëŠ” SFSì—ì„œ ì„ íƒí•˜ì§€ ì•ŠëŠ”ë‹¤.

### Plus-L Minus-R Selection (`LRS`)
- BDSì™€ ê°™ì´ ê°™ì´ ì‚¬ìš©í•˜ëŠ”ë°, ê³„ìˆ˜ê°€ Lê³¼ Rë¡œ ë‹¤ë¥´ë‹¤.
- ë‹¨ì ì€ caseë§ˆë‹¤, L ë° Rì˜ ìµœì ê°’ì„ ì˜ˆì¸¡í•  ë°©ë²•ì´ ì—†ë‹¤. (heuristics)
- ex) L > R ì¼ ê²½ìš°,
<br> Empty subsetì—ì„œ ì‹œì‘í•˜ë©°, Lê°œ ì¶”ê°€ - Rê°œ ì œê±° ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.

### `Sequential Floating Forward Selection` (`SFFS`)
- Empty subsetì—ì„œ ì‹œì‘í•œë‹¤.
- Forward ì§„í–‰í•˜ì—¬ featureê°€ ì¶”ê°€ë  ë•Œ, ì¶”ê°€ëœ feature ì™¸ ë‹¤ë¥¸ ê²ƒì„ ì œê±°í•˜ê³  ë” ì¢‹ì€ ì„±ëŠ¥ì˜ subsetì´ ìˆëŠ”ì§€ ë¹„êµí•œë‹¤. ê³„ì† backward ê²€ì¦í•˜ë‹¤ê°€ ë” optimalí•œ subsetì´ ì—†ë‹¤ë©´ ë‹¤ì‹œ forward ì§„í–‰í•œë‹¤.

### `Sequential Floating Backward Selection` (`SFBS`)
- Full subsetì—ì„œ ì‹œì‘í•œë‹¤.
- Backward ì§„í–‰í•˜ì—¬ featureê°€ ì œê±°ë  ë•Œ, ì œê±°ëœ featureì™¸ ë‹¤ë¥¸ ê²ƒì„ ì¶”ê°€í•˜ê³  ë” ì¢‹ì€ ì„±ëŠ¥ì˜ subsetì´ ìˆëŠ”ì§€ ë¹„êµí•œë‹¤. ê³„ì† foward ê²€ì¦í•˜ë‹¤ê°€ ë” optimalí•œ subsetì´ ì—†ë‹¤ë©´ ë‹¤ì‹œ backward ì§„í–‰í•œë‹¤.
<br><img width="500" src="https://user-images.githubusercontent.com/89369520/141403008-8a1bde7a-bdc3-49d9-abfc-7e44e625ef0b.png">

## 2. Linear Transformation
### Feature Extraction
- xì— Wë¥¼ ê³±í•´ì„œ Rdë¥¼ Rd'ìœ¼ë¡œ ì°¨ì›ì„ ë‚®ì¶°ë³´ì.
1. Principal Component Anaiysls, `ì£¼ ì„±ë¶„ ë¶„ì„(PCA)`
<br> dataë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” ê²ƒ ì°¾ê¸°
2. Linear Discriminant Analysis, `ì„ í˜• íŒë³„ ë¶„ì„(LDA)`
<br> ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ êµ¬ë¶„í•˜ëŠ” ê²ƒ ì°¾ê¸°

## 1. Principal Component Analysis
### `PCA, ì£¼ ì„±ë¶„ ë¶„ì„`
- ì•„ì´ë””ì–´ : ì € ì°¨ì› ê³µê°„ì—ì„œ ê°€ì¥ ì •í™•í•œ ë°ì´í„° í‘œí˜„ ì°¾ê¸°
- Dataì˜ varianceê°€ ê°€ì¥ ì˜ í‘œí˜„ë˜ëŠ”, ê°€ì¥ í° ë°©í–¥ìœ¼ë¡œ projection í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.
<br> projection í›„ ì¢Œí‘œê³„ë¥¼ ë³€í™˜í•œë‹¤. ë˜í•œ ìƒˆ ë°ì´í„° yì˜ Varì´ ì´ì „ ë°ì´í„° xì˜ varì™€ ë™ì¼í•˜ë‹¤.
<br><img width="300" src="https://user-images.githubusercontent.com/89369520/141425347-84d88d9e-2e2e-4b5f-b234-8620754085ce.png">

### Shift by mean
- dataì—ì„œ sample mean ë¹¼ê¸°
<br><img width="200" src="https://user-images.githubusercontent.com/89369520/141425769-56324cb6-401e-49be-8012-f43086c22da9.png">
<br>PCAëŠ” ë¶„ì‚°ìœ¼ë¡œ ì´ì•¼ê¸° í•˜ë¯€ë¡œ dataì˜ ê°’ì´ ì–¼ë§ˆë‚˜ í°ì§€ëŠ” ì˜ë¯¸ì—†ë‹¤. 0, 0ìœ¼ë¡œ ì¤Œì‹¬ì„ ì˜®ê²¨ì£¼ì.
<br><img width="300" src="https://user-images.githubusercontent.com/89369520/141425448-8b3f7227-cd49-4127-8f01-5436a4a7aff9.png">

### Derivation
- ì°¨ì› ğ‘š < dë¥¼ ê°–ëŠ” ë¶€ë¶„ê³µê°„ subspace Wì—ì„œ Dì˜ ê°€ì¥ ì •í™•í•œ í‘œí˜„ì„ ì°¾ëŠ”ë‹¤.
- í–‰ë ¬ì´ë€ ì„ í˜•ë³€í™˜ì´ê³ , í•˜ë‚˜ì˜ ë²¡í„° ê³µê°„ì„ ì„ í˜•ì ìœ¼ë¡œ ë‹¤ë¥¸ ë²¡í„° ê³µê°„ìœ¼ë¡œ mappingí•˜ëŠ” ê¸°ëŠ¥ì„ ê°€ì§„ë‹¤. ë”°ë¼ì„œ Wì— ìˆëŠ” ì–´ë– í•œ ë²¡í„°ë„ Î£aieië¡œ ì“¸ ìˆ˜ ìˆìœ¼ë©°, x1ì€ Î£a1iei ë¡œ ì“¸ ìˆ˜ ìˆë‹¤.(a11e1+a12e2ì´ë¯€ë¡œ) ê·¸ë˜ì„œ ì¬êµ¬ì„±ì—ì„œ ì˜¤ëŠ” ì˜¤ë¥˜ì˜ í•©ì„ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤.
<br><img width="300" src="https://user-images.githubusercontent.com/89369520/141438774-c5e81119-884e-45c3-863c-3a1c9dd046c7.png">
- ì „ì²´ ì—ëŸ¬ëŠ” ë‹¤ìŒê³¼ ê°™ìœ¼ë©°, ìµœì†Œí™” í•˜ëŠ”ë° ëª©ì ì´ ìˆë‹¤. (ì—¬ê¸°ì„œ SëŠ” Covariance matrixì—ì„œ n-1ì´ ê³±í•´ì§„ ê°’ì´ë‹¤.)
<br><img width="300" src="https://user-images.githubusercontent.com/89369520/141438984-d3307985-857a-49d7-9e19-1375aca105dc.png">
- 





